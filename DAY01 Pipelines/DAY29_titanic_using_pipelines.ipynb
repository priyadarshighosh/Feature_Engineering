{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pds0smRMf0cz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt     #importing the basic data Science Libraries like numpy pandas matplotlib and seaborn\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split               #for splitting the data into test and training data\n",
        "from sklearn.compose import ColumnTransformer                       #for transforming the columns\n",
        "from sklearn.impute import SimpleImputer                             #for imputing the missing values\n",
        "from sklearn.preprocessing import OneHotEncoder                      #one hot encoding\n",
        "from sklearn.preprocessing import MinMaxScaler                        #standard scaling\n",
        "from sklearn.pipeline import Pipeline,make_pipeline                    #here we wont use pipelines\n",
        "from sklearn.feature_selection import SelectKBest,chi2                 #feature selection\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "source": [
        "from google.colab import files    # we are importing the file from the device\n",
        "uploaded = files.upload()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "BDVlDCcgBw00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('Titanic-Dataset.csv')   #fitting the data in the df dataframe\n",
        "df.head()"
      ],
      "metadata": {
        "id": "iPOR-B8ghwZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['PassengerId', 'Name', 'Ticket' , 'Cabin'], inplace=True , axis=1)"
      ],
      "metadata": {
        "id": "4YWgXKEQQeuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df.drop('Survived', axis=1),\n",
        "                                                    df['Survived'], test_size=0.2,\n",
        "                                                    random_state=42)"
      ],
      "metadata": {
        "id": "EoAHclRISLBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head()"
      ],
      "metadata": {
        "id": "x-1ivIHLSTJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.head()"
      ],
      "metadata": {
        "id": "tHP7P5nPSYXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPUTATION"
      ],
      "metadata": {
        "id": "41LvxCZaM2RF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imputation transformer\n",
        "trf1=ColumnTransformer([\n",
        "  ('impute_age',SimpleImputer(),[2]),                               # imporitng the age instead of writing the column name we gave the column positon\n",
        "    ('impute_embarked',SimpleImputer(strategy='most_frequent'),[6]) #6th position of the arrary is for embarked\n",
        "],remainder='passthrough')"
      ],
      "metadata": {
        "id": "iTocue10RxSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "most frequent means Median , it replaces the missing values with Median"
      ],
      "metadata": {
        "id": "ux0jULC8Osu8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ONE HOT ENCODING"
      ],
      "metadata": {
        "id": "BM0Ze9doMz6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#one hot encoding\n",
        "trf2=ColumnTransformer([\n",
        "    ('ohe_sex_embarked',OneHotEncoder(sparse=False,handle_unknown ='ignore', drop='first'),[1,6])  # one hot encoding for sex in pos 1  and embarked in pos 6\n",
        "],remainder='passthrough')                                                                         #drop first bascially makes the One-hot-Encoding itnto DUMMY ENCODING\n",
        "                                                                                           #we may not use drop-first as in decesion tree model it doesnot really matter"
      ],
      "metadata": {
        "id": "XEznUWm0RxA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sparse=False: This parameter specifies that the output should be a dense array instead of a sparse matrix.\n",
        "\n",
        "\n",
        "drop='first': This parameter specifies that for each feature, the first category should be dropped to avoid multicollinearity in the features."
      ],
      "metadata": {
        "id": "rvGSOfs0OeV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "remainder='passthrough': This parameter specifies what to do with columns that are not explicitly transformed. Here, 'passthrough' means that those columns will be passed through without any transformation."
      ],
      "metadata": {
        "id": "N5UTi_3nOnTU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "FEATURE SCALING"
      ],
      "metadata": {
        "id": "Mn7eMvwBM4kG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feature scaling\n",
        "trf3=ColumnTransformer([\n",
        "    ('scale',MinMaxScaler(),slice(0,10))       #min max scaling  on all columns in the range 0 to 8 we have not done it on categorical data but we did it anyways\n",
        "])"
      ],
      "metadata": {
        "id": "JVoXckl0gzja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Z-Score Normalization , it shifts the value from  0 to 1"
      ],
      "metadata": {
        "id": "AYUoJhXiTjOU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using MinMax Scaler instead of Standard Scaler  as we are also doing Feature Selection - In feature selection we always use MinMax"
      ],
      "metadata": {
        "id": "dmF5RAHtT3Xv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "FEATURE SELECTION"
      ],
      "metadata": {
        "id": "93n_LKvOM647"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#feature selection\n",
        "trf4=SelectKBest(score_func=chi2,k=7)     #we dont have to know everything about it right now but future mein we gonna get to know about it more"
      ],
      "metadata": {
        "id": "qrflx-BhpqR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basicallt we are using the top 8 columns of the data"
      ],
      "metadata": {
        "id": "-mwcaLIsajCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINING THE MODEL"
      ],
      "metadata": {
        "id": "hPSXK2CONJPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trf5=DecisionTreeClassifier()"
      ],
      "metadata": {
        "id": "Ban0fiCKNEf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the Pipelines"
      ],
      "metadata": {
        "id": "GbWQp7TDarO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we did all the parts separatly now we going to Join all the parts\n",
        "\n",
        "1.   Imputation\n",
        "2.   One-Hot Encoding\n",
        "3.   Feature Scaling\n",
        "4.   Feature Selection  \n",
        "\n"
      ],
      "metadata": {
        "id": "C4eZAPYKbJFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from  sklearn.pipeline import Pipeline,make_pipeline\n",
        "\n",
        "from os import pipe\n",
        "pipe= Pipeline([\n",
        "    ('trf1',trf1),                       #we Joiend everyhting together\n",
        "    ('trf2',trf2),\n",
        "    ('trf3',trf3),\n",
        "    ('trf4',trf4),\n",
        "    ('trf5',trf5)\n",
        "])"
      ],
      "metadata": {
        "id": "nEZ7yEezaxEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code we are sending the list of tuple\n",
        "1st we are sending the transformer and creating the pipeline object"
      ],
      "metadata": {
        "id": "o34EGp7xbgs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ALTERNATIVE WAY\n",
        "\n",
        "#pipe=make_pipeline(trf1,trf2,trf3,trf4,trf5)       #we passed just the names of the objects thats it\n"
      ],
      "metadata": {
        "id": "vEDCCR-Gc1Av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pipeline requires naming the steps , make_pipeline doesnot\n",
        "\n",
        "(same applies to Column Transformer vs make_column_transformer)"
      ],
      "metadata": {
        "id": "5ANhHKrqc7a4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training the pipeline\n",
        "\n",
        "pipe.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "SQuDzzkOdnl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VISUAL DIAGRAM"
      ],
      "metadata": {
        "id": "Ga32Cc4Yfba0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#disply\n",
        "\n",
        "from sklearn import set_config\n",
        "set_config(display='diagram')"
      ],
      "metadata": {
        "id": "sPocCzU_flQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Few Learning"
      ],
      "metadata": {
        "id": "BOCALTEQeBHZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "if we have 3 steps imputation , OHE and scaling , we didnot have any algorithm then We wont just call the fit function\n",
        "\n",
        "We would call fit_transform function\n",
        "Here we do the 2 things at one go as we just did preprossing we didnot do model training\n",
        "so we do 2 things at once so that after we can call the predict function"
      ],
      "metadata": {
        "id": "E_89DAwJeKmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TYPES OF PIPELINES\n",
        "\n",
        "WITH ALGORITHM WE USE FIT AND PREDICT\n",
        "\n",
        "WITHOUT ALGORITHM WE USE FIT AND TRANSFORM"
      ],
      "metadata": {
        "id": "cY5J9gvUfBPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline EXPLORATION  "
      ],
      "metadata": {
        "id": "ILc02K5Sf28a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.named_steps       #this tells us all the steps that this pipeline follows"
      ],
      "metadata": {
        "id": "_klQ7Oacf--m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.named_steps['trf1'].transformers_"
      ],
      "metadata": {
        "id": "g17UiE-lgWYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this tells us 2nd postion we imputed for age and 6th position we imputed for embarked and passed through the others , it is basically describes the transformers"
      ],
      "metadata": {
        "id": "RQbQTvuJghpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.named_steps['trf1'].transformers_[0]  #gives the age part of the imputer that is the first part"
      ],
      "metadata": {
        "id": "umBxtPmRg17M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.named_steps['trf1'].transformers_[0][1]     #it shows that this has the simple imputer object"
      ],
      "metadata": {
        "id": "bf_wwTLJhGoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.named_steps['trf1'].transformers_[0][1].statistics_    #statistics is the attribute"
      ],
      "metadata": {
        "id": "-CCJk0A4hWVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we did it for age we can also do it for the embarked column"
      ],
      "metadata": {
        "id": "9uTMoB9ahhof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.named_steps['trf1'].transformers_[1][1].statistics_"
      ],
      "metadata": {
        "id": "FPTrg152hnL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "S comes most frequently"
      ],
      "metadata": {
        "id": "y1SVAHKMhz8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The above helps a lot in Backtracking and Debugging\n",
        "\n",
        "# We can do experiments with  tranformers and do Post-mortem of the code"
      ],
      "metadata": {
        "id": "jm7W92qJiAnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can do better job than kolkata police at it"
      ],
      "metadata": {
        "id": "0ZgYY3Y4iQXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction  and Accuracy time!!!"
      ],
      "metadata": {
        "id": "Px1VMJFMiW9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=pipe.predict(X_test)\n"
      ],
      "metadata": {
        "id": "wMA5PfvTingD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "id": "lsJa_TeeiwtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "metadata": {
        "id": "9bl0DmFRi_FA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross Validation with pipeline"
      ],
      "metadata": {
        "id": "IUTpa1N3jFEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "cross_val_score(pipe,X_train,y_train,cv=5,scoring='accuracy').mean()"
      ],
      "metadata": {
        "id": "xT8pJaFojKYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 60 perc after Cross Validation\n",
        "\n",
        "#Horrendous reseult after 2 days of work"
      ],
      "metadata": {
        "id": "22c4UHJTjl2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GridSearch using Pipeline"
      ],
      "metadata": {
        "id": "QXlWUvYSjw_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HYPER PARAMETER TRAINING WE WILL LEARN ABOUT THIS IN FUTURE\n",
        "\n",
        "2 PARTS OF ML TUNING THE FEATURE AND TUNING THE MODEL\n",
        "\n",
        "like depth of the Des"
      ],
      "metadata": {
        "id": "Yo99BhAEj4oI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#gridsearch CV\n",
        "params = {\n",
        "\n",
        "          'trf5__max_depth':[1,2,3,4,5,None]     # for these 6 values it will try and gridsearch cv automatically chooses that\n",
        "}                                                 #tr5f is the name of the model\n"
      ],
      "metadata": {
        "id": "4EpO0dGij1qZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "grid=GridSearchCV(pipe,params,cv=5,scoring='accuracy')\n",
        "grid.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "9hYs4NZOmm-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid.best_score_          #accuracy after GridSearch CV"
      ],
      "metadata": {
        "id": "7QSKG3VqmH3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We getting kind of the same accuracy   60%   so ya\n",
        "\n",
        "Learning done ."
      ],
      "metadata": {
        "id": "IDp5mLQrmLSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grid.best_params_"
      ],
      "metadata": {
        "id": "bQhXtmekmsSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Exporting Pipelines"
      ],
      "metadata": {
        "id": "_seSRgeFUru1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickle.dump(pipe,open('pipe.pkl','wb'))    #what if we wanna use this in production code"
      ],
      "metadata": {
        "id": "N0e0i79YVrFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we dont have to bring all the transformers separatly , it is really stored in the pipelines so we just have to do it once\n",
        "\n",
        "Makes our work soo much easier"
      ],
      "metadata": {
        "id": "6P2ipIynV3ba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe=pickle.load(open('pipe.pkl','rb'))  #loading pipeline"
      ],
      "metadata": {
        "id": "cWkI7an4WTg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#User input and prediction\n",
        "\n",
        "test_input2 = np.array([3, 'female', 32.0, 0, 0, 11.5, 'S'],dtype=object).reshape(1,7)\n",
        "\n",
        "pipe.predict(test_input2)"
      ],
      "metadata": {
        "id": "hC-30AB9WaWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction is THE PERSON WILL Not survive\n",
        "\n",
        "\n",
        "If we change some code during the transformation , we dont have to make any changes in the pickle part and it takes the ENTIRE PIPELINE\n",
        "\n",
        "AND IN THE ENTIRE PIPELINE changes will be automatically aggregated."
      ],
      "metadata": {
        "id": "u1_d8c3nXCqM"
      }
    }
  ]
}